# Attention Model for Neural Machine Translation: English to French

## Under Construction

Neural Machine Translation (NMT) refers to the use of neural networks, to translate one form of information into other.

Some common examples include language translation, speech to text and image captioning.

Most common neural network architectures used for machine translation are Recurrent Neural Networks (RNNs), and its variations including GRUs and LSTMs. This task falls under the sequence-to-sequence category, and traditionally, the go to model for such a task used to be encoder-decoder models.

![alt text](https://raw.githubusercontent.com/sarangzambare/nmt_attention/master/png/encoder_decoder.png)


<common architectures: encoder-decoder>

<B-RNN working>

<problem with common architectures>


<attention modelling working>


<demonstration>


<attention matrix>
